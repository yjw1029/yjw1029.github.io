---
layout: post
title:  "On the Vulnerability of Value Alignment in Open-Access LLMs"
date:   2023-10-13 00:00:00 +00:00
image: /images/reverse-alignment.png
categories: research
author: "Jingwei Yi"
authors: "<strong>Jingwei Yi*</strong>, Rui Ye*, Qisi Chen, Bin Zhu, Siheng Chen, Defu Lian, Guangzhong Sun, Xing Xie, Fangzhao Wu"
venue: "ACL 2024 Findings"
paper: https://openreview.net/pdf?id=NIouO0C0ex
---
We reveal the vulnerabilities of large language models (LLMs) to reverse alignment attacks and introduce reverse supervised fine-tuning (RSFT) and reverse preference optimization (RPO) as efficient attack methods. Our research underscores the limitations of current value alignment methods and emphasizes the need for robust solutions to counteract malicious fine-tuning.