---
layout: post
title:  "On the Vulnerability of Value Alignment in Open-Access LLMs"
date:   2023-10-13 00:00:00 +00:00
image: /images/reverse-alignment.png
categories: research
author: "Jingwei Yi"
authors: "<strong>Jingwei Yi*</strong>, Rui Ye*, Qisi Chen, Bin Zhu, Siheng Chen, Defu Lian, Guangzhong Sun, Xing Xie, Fangzhao Wu"
venue: "ACL 2024 Findings"
paper: https://openreview.net/pdf?id=NIouO0C0ex
---
We systematically evaluate the robustness of LLMs to indirect prompt injection attacks and propose several defense techniques to mitigate the risks.